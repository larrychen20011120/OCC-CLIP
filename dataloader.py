import json, time, random, os, GPUtil

import warnings
warnings.filterwarnings("ignore", category=UserWarning)
import shutil
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
from PIL import Image
from torch import nn
from sklearn.metrics import roc_auc_score, accuracy_score
import torchvision.transforms as tt
from torch.utils.data import DataLoader
import albumentations as A
from clip import clip
from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer
from torch.nn.functional import interpolate
from torch.optim.lr_scheduler import _LRScheduler

PATH = "/homes/55/jindong/multiagent/OCC-CLIP" # path to json files
REPO_DIR = "/scratch/local/ssd/jindong/fengyuan" # path to data

COCO_TEST_PATH = f'{REPO_DIR}/train_val_test/MSCOCO_RS/'
GALIP_COCO_PATH = f'{REPO_DIR}/train_val_test/coco_galip256'
GLIDE_PATH = f'{REPO_DIR}/train_val_test/glide'
LDM_PATH = f'{REPO_DIR}/train_val_test/latent_diffusion'
SDV1_PATH = f'{REPO_DIR}/train_val_test/stable_diffusion_v1'
VQ_PATH = f'{REPO_DIR}/train_val_test/VQ_diffusion'
GauGan_O_PATH = f'{REPO_DIR}/train_val_test/GauGan_O'
Progan_O_PATH = f'{REPO_DIR}/train_val_test/Progan_O'
StyleGan2_O_PATH = f'{REPO_DIR}/train_val_test/StyleGan2_O'


path_map ={
        'gaugan_o': GauGan_O_PATH, 
        'progan_o': Progan_O_PATH, 
        'stylegan2_o': StyleGan2_O_PATH,
        "coco_test": COCO_TEST_PATH,
        "galip_coco": GALIP_COCO_PATH,
        "glide" : GLIDE_PATH,
        "ldm":LDM_PATH,
        "sdv1":SDV1_PATH,
        "vq":VQ_PATH,
}


def data_label(split_name, opt, one_value, zero_value):
    print("Loading data and labels...")
    dataset = {split_name:[]}
    labels = {split_name:[]}

    if split_name == 'train':
        dataset = json.load(open(f"{PATH}/dataset/TrainData{opt.train_set}.json", "r"))
        num = 50
    elif split_name == 'test':
        dataset = json.load(open(f"{PATH}/dataset/TestData_{zero_value[0]}_{one_value[0]}.json", "r"))
        num = 1000
    elif split_name == 'val':
        dataset = json.load(open(f"{PATH}/dataset/ValData.json", "r"))
        num = 500 

    labels[split_name].extend([0]*num)
    labels[split_name].extend([1]*num)

    dataset[split_name] = [os.path.join(REPO_DIR, item) for item in dataset[split_name]]

    print("Data and labels loaded.")
    return dataset, labels

# model_aware_load(opt.train_target,'train', opt.train_real, opt.exp, opt.n_exp, transform)
def model_aware_load(one_value, split, zero_value, opt, transform): #####################
    print("Loading data generated by : ", zero_value, one_value)
    # path_map_list = [path_map[x.lower()] for x in zero_value]
    # path_map_list.extend([path_map[x.lower()] for x in one_value])

    # print(path_map_list)
    data, labels = data_label(split, opt, one_value, zero_value)
    
    dataset = task1Dataset(split, data, labels, opt, transform)
    return dataset

class task1Dataset(Dataset): 
    
    def __init__(self, split, data, label, opt, transform):
        self.split = split
        self.opt = opt
        print("init dataset for : ", self.split, len(data[self.split]),"images")
        self.data = [Image.open(img).convert("RGB") for img in tqdm(data[self.split])]
        self.label = torch.tensor(label[self.split],dtype=torch.long)
        self.transform_norm = transform
    
    def __getitem__(self, idx):
        assert type(idx) == int, idx
        image = self.data[idx]   # Image.open( self.data[idx]).convert("RGB") 
        label = self.label[idx]
        image = self.transform_norm(image)

        return image, label
    
    def __len__(self) -> int:
        return len(self.data)

def get_available_gpus(min_memory_available):
    """Returns a list of IDs for GPUs with more than min_memory_available MB of memory."""
    GPUs = GPUtil.getGPUs()
    available_gpus = [gpu.id for gpu in GPUs if gpu.memoryFree > min_memory_available]
    return available_gpus

def test_non_overlap(dataset):
    #for one_value in model_list:
        # other = COCO_PATH
    train_dataset = dataset['train']
    valid_dataset = dataset['val']
    test_dataset = dataset['test']
    #print(train_dataset)
    # print(train_dataset.data[0])
    # print(train_dataset[0])
    union_num = len(set(train_dataset).union(set(valid_dataset)).union(set(test_dataset)))
    epxected_num = len(train_dataset)+len(valid_dataset)+len(test_dataset)
    print(len(set(train_dataset).intersection(set(valid_dataset))), \
          len(set(train_dataset).intersection(set(test_dataset))), \
          len(set(valid_dataset).intersection(set(test_dataset))))
    # ensure non duplicate in train, valid, test
    # assert set(os.listdir(COCO_PATH)).intersection(set(os.listdir(GALIP_CC12M_PATH))) == set()
    assert len(train_dataset) == len(set(train_dataset)), f" train has duplicate"
    assert len(valid_dataset) == len(set(valid_dataset)), f"valid has duplicate"
    assert len(test_dataset) == len(set(test_dataset)), f"test has duplicate"
    print("pass test_duplicate")
    # assert set(train_dataset.data).isdisjoint(set(valid_dataset.data)), f"{gen} train and valid overlap"
    assert set(train_dataset).isdisjoint(set(test_dataset)), f"train and test overlap"
    assert set(valid_dataset).isdisjoint(set(test_dataset)), f" valid and test overlap"
    assert union_num == epxected_num, f"in fact {union_num} expect {epxected_num}"
    print("pass test_non_overlap")